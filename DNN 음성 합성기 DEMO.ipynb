{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실시간 다화자 다감정 음성 합성 시스템 시연",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chj1330/realtimeMultiSpeakerMultiEmotionTTS/blob/main/DNN%20%EC%9D%8C%EC%84%B1%20%ED%95%A9%EC%84%B1%EA%B8%B0%20DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuhqhYSToxl7"
      },
      "source": [
        "# 실시간 다화자 다감정 음성 합성 시스템 시연\n",
        "\n",
        "\n",
        "Multi-speaker Emotional FastSpeech + ParallelWaveGAN \n",
        "\n",
        "Reference\n",
        "- ESPnet: https://github.com/espnet/espnet\n",
        "- ParallelWaveGAN: https://github.com/kan-bayashi/ParallelWaveGAN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e_i_gdgAFNJ"
      },
      "source": [
        "## Install & Setup (3~4분 소요)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjJ5zkyaoy29",
        "outputId": "3819a59f-4c23-44b3-bd55-3fb428b3b3ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "# install minimal components\n",
        "!pip install -q parallel_wavegan PyYaml unidecode ConfigArgparse torch==1.5.0+cu101 torchvision==0.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone -q https://github.com/kan-bayashi/espnet.git -b fix_import\n",
        "!cd espnet && git fetch && git checkout -b v.0.7.0 4ad3247c850bb6696e4e2c3f7633c0153463dded"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 14.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 703.8MB 25kB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6MB 37kB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 17.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 317kB 20.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[?25h  Building wheel for parallel-wavegan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ConfigArgparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kaldiio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Switched to a new branch 'v.0.7.0'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1a5CgX1AHXJ"
      },
      "source": [
        "### Download pretrained feature generation model\n",
        "\n",
        "Multi-speaker Emotional FastSpeech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX0Kmo72POfY",
        "outputId": "393a6a4f-7a11-448b-c67e-4237a9641515",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# download pretrained model\n",
        "import os\n",
        "if not os.path.exists(\"downloads/ptFastSpeech\"):\n",
        "    !./espnet/utils/download_from_google_drive.sh \\\n",
        "       https://drive.google.com/open?id=14JKGJ5lfM6ATaavDAlJyCGA81BAden9x downloads/ptFastSpeech tar.gz\n",
        "\n",
        "# set path\n",
        "trans_type = \"char\"\n",
        "dict_path = \"downloads/ptFastSpeech/data/lang_1char/EMO_char_train_no_dev_units.txt\"\n",
        "model_path = \"downloads/ptFastSpeech/exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/results/model.loss.best\"\n",
        "cmvn_path = \"downloads/ptFastSpeech/data/EMO_char_train_no_dev/cmvn.ark\"\n",
        "\n",
        "print(\"Sucessfully finished download.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-12 08:45:41--  https://drive.google.com/uc?export=download&id=14JKGJ5lfM6ATaavDAlJyCGA81BAden9x\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.126.100, 108.177.126.113, 108.177.126.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.126.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘downloads/ptFastSpeech/CYPQRi.tar.gz’\n",
            "\n",
            "downloads/ptFastSpe     [ <=>                ]   3.18K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-12 08:45:41 (36.8 MB/s) - ‘downloads/ptFastSpeech/CYPQRi.tar.gz’ saved [3256]\n",
            "\n",
            "\n",
            "gzip: stdin: not in gzip format\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3256    0  3256    0     0  14800      0 --:--:-- --:--:-- --:--:-- 14733\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2266      0 --:--:-- --:--:-- --:--:--  2266\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  376M    0  376M    0     0  63.5M      0 --:--:--  0:00:05 --:--:-- 89.7M\n",
            "nets/e2e_tts_fastspeech_sgst2.py\n",
            "exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/outputs_model.loss.best_decode_stemb1.0/EMO_char_train_no_dev/emotion.scp\n",
            "exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/results/\n",
            "conf/train_fastspeech.sgst2.spkid.yaml\n",
            "data/EMO_char_train_no_dev/\n",
            "nets/e2e_tts_tacotron2.py\n",
            "conf/train_pytorch_tacotron2_gst.yaml\n",
            "nets/\n",
            "exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/outputs_model.loss.best_decode_stemb1.0/\n",
            "exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/outputs_model.loss.best_decode_stemb1.0/EMO_char_train_no_dev/\n",
            "exp/LMY_char_train_no_dev_pytorch_train_pytorch_tacotron2_gst/results/\n",
            "data/grp/\n",
            "exp/LMY_char_train_no_dev_pytorch_train_pytorch_tacotron2_gst/results/model.loss.best\n",
            "nets/supervisedgst.py\n",
            "exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/outputs_model.loss.best_decode_stemb1.0/EMO_char_train_no_dev/emotion.ark\n",
            "exp/LMY_char_train_no_dev_pytorch_train_pytorch_tacotron2_gst/\n",
            "nets/duration_calculator.py\n",
            "data/grp/table.txt\n",
            "conf/\n",
            "exp/LMY_char_train_no_dev_pytorch_train_pytorch_tacotron2_gst/results/model.json\n",
            "data/lang_1char/\n",
            "data/EMO_char_train_no_dev/cmvn.ark\n",
            "decode.yaml\n",
            "exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/results/model.loss.best\n",
            "data/lang_1char/EMO_char_train_no_dev_units.txt\n",
            "exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/\n",
            "data/\n",
            "exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/results/model.json\n",
            "exp/\n",
            "nets/gst.py\n",
            "Sucessfully downloaded tar.gz file from https://drive.google.com/open?id=14JKGJ5lfM6ATaavDAlJyCGA81BAden9x\n",
            "Sucessfully finished download.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwc7JXD_dAy8"
      },
      "source": [
        "### Download pretrained vocoder model\n",
        "\n",
        "Parallel WaveGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQDFNuQ2dK-M",
        "outputId": "0fa9ea84-3039-4410-9e77-4d6f24032382",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "# download pretrained model\n",
        "import os\n",
        "if not os.path.exists(\"downloads/ptParallelWavegan\"):\n",
        "    !./espnet/utils/download_from_google_drive.sh \\\n",
        "        https://drive.google.com/open?id=1GVVG9lw6Aq2a-C6au7KCMtp186GtsXrk downloads/ptParallelWavegan tar.gz\n",
        "\n",
        "# set path\n",
        "vocoder_path = \"downloads/ptParallelWavegan/checkpoint-255000steps.pkl\"\n",
        "vocoder_conf = \"downloads/ptParallelWavegan/config.yml\"\n",
        "vocoder_stat =  \"downloads/ptParallelWavegan/stats.h5\"\n",
        "\n",
        "print(\"Sucessfully finished download.\")\n",
        "\n",
        "# add path\n",
        "import sys\n",
        "sys.path.append(\"espnet\")\n",
        "sys.path.append(\"downloads/ptFastSpeech\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-12 08:45:53--  https://drive.google.com/uc?export=download&id=1GVVG9lw6Aq2a-C6au7KCMtp186GtsXrk\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.119.101, 108.177.119.102, 108.177.119.138, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.119.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-00-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d5ua3kau0v2bvaa5k2u0sskk8a9b5n2k/1602492300000/07266571289902820001/*/1GVVG9lw6Aq2a-C6au7KCMtp186GtsXrk?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-10-12 08:45:56--  https://doc-00-9s-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/d5ua3kau0v2bvaa5k2u0sskk8a9b5n2k/1602492300000/07266571289902820001/*/1GVVG9lw6Aq2a-C6au7KCMtp186GtsXrk?e=download\n",
            "Resolving doc-00-9s-docs.googleusercontent.com (doc-00-9s-docs.googleusercontent.com)... 172.217.218.132, 2a00:1450:4013:c08::84\n",
            "Connecting to doc-00-9s-docs.googleusercontent.com (doc-00-9s-docs.googleusercontent.com)|172.217.218.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/gzip]\n",
            "Saving to: ‘downloads/ptParallelWavegan/wKHJdB.tar.gz’\n",
            "\n",
            "downloads/ptParalle     [  <=>               ]  15.09M  20.2MB/s    in 0.7s    \n",
            "\n",
            "2020-10-12 08:45:57 (20.2 MB/s) - ‘downloads/ptParallelWavegan/wKHJdB.tar.gz’ saved [15822934]\n",
            "\n",
            "checkpoint-255000steps.pkl\n",
            "config.yml\n",
            "stats.h5\n",
            "Sucessfully downloaded tar.gz file from https://drive.google.com/open?id=1GVVG9lw6Aq2a-C6au7KCMtp186GtsXrk\n",
            "Sucessfully finished download.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaSyEKBWAK7H"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8JXOfRfqMFN",
        "outputId": "a5902000-34f2-4cd7-df96-702fe657d18e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# define device\n",
        "import torch\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# define E2E-TTS model\n",
        "from argparse import Namespace\n",
        "from os.path import join\n",
        "from espnet.asr.asr_utils import get_model_conf\n",
        "from espnet.asr.asr_utils import torch_load\n",
        "from espnet.utils.dynamic_import import dynamic_import\n",
        "idim, odim, train_args = get_model_conf(model_path)\n",
        "model_class = dynamic_import(train_args.model_module)\n",
        "model = model_class(idim, odim, train_args)\n",
        "torch_load(model_path, model)\n",
        "model = model.eval().to(device)\n",
        "inference_args = Namespace(**{\n",
        "    \"threshold\": 0.5,\"minlenratio\": 0.0, \"maxlenratio\": 10.0,\n",
        "    # Only for Tacotron 2\n",
        "    \"use_attention_constraint\": True, \"backward_window\": 1,\"forward_window\":3,\n",
        "    # Only for fastspeech (lower than 1.0 is faster speech, higher than 1.0 is slower speech)\n",
        "    \"fastspeech_alpha\": 1.0,\n",
        "    })\n",
        "\n",
        "# define neural vocoder\n",
        "import yaml\n",
        "import parallel_wavegan.models\n",
        "with open(vocoder_conf) as f:\n",
        "    config = yaml.load(f, Loader=yaml.Loader)\n",
        "vocoder_class = config.get(\"generator_type\", \"ParallelWaveGANGenerator\")\n",
        "vocoder = getattr(parallel_wavegan.models, vocoder_class)(**config[\"generator_params\"])\n",
        "vocoder.load_state_dict(torch.load(vocoder_path, map_location=\"cpu\")[\"model\"][\"generator\"])\n",
        "vocoder.remove_weight_norm()\n",
        "vocoder = vocoder.eval().to(device)\n",
        "if config[\"generator_params\"][\"out_channels\"] > 1:\n",
        "    from parallel_wavegan.layers import PQMF\n",
        "    pqmf = PQMF(config[\"generator_params\"][\"out_channels\"]).to(device)\n",
        "\n",
        "from espnet.transform.cmvn import CMVN\n",
        "\n",
        "cmvn = CMVN(cmvn_path, norm_means=True, norm_vars=True, reverse=True)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import h5py\n",
        "scaler = StandardScaler()\n",
        "with h5py.File(vocoder_stat, \"r\") as f:\n",
        "    scaler.mean_ = f[\"mean\"][()]\n",
        "    scaler.scale_ = f['scale'][()]\n",
        "scaler.n_features_in_ = scaler.mean_.shape[0]\n",
        "# define text frontend\n",
        "with open(dict_path) as f:\n",
        "    lines = f.readlines()\n",
        "lines = [line.replace(\"\\n\", \"\").split(\" \") for line in lines]\n",
        "char_to_id = {c: int(i) for c, i in lines}\n",
        "\n",
        "from unidecode import unidecode\n",
        "import re\n",
        "class Text2Grp(object):\n",
        "    def __init__(self, fn_grptable):\n",
        "        self.grptable = grptable = self.loadTABLE(fn_grptable)\n",
        "        self.INITIALS = self.grptable[0].split(' ')\n",
        "        self.G_INITIALS = grptable[1].split(' ')\n",
        "        self.MEDIALS = grptable[2].split(' ')\n",
        "        self.G_MEDIALS = grptable[3].split(' ')\n",
        "        self.FINALS = grptable[4].split(' ')\n",
        "        self.G_FINALS = grptable[5].split(' ')\n",
        "        self.SPECIALS = grptable[6].split(' ')\n",
        "        self.SPECIALS.append(' ')\n",
        "        self.CHARACTERS = self.INITIALS + self.MEDIALS + self.FINALS + self.SPECIALS\n",
        "        self.flag_specials = False\n",
        "\n",
        "        if len(self.INITIALS) != len(self.G_INITIALS):\n",
        "            print(\"Error: character_INITIALS and grapheme_INITIALS length mismatch\")\n",
        "            sys.exit(1)\n",
        "        if len(self.MEDIALS) != len(self.G_MEDIALS):\n",
        "            print(\"Error: character_MEDIALS and grapheme_MEDIALS length mismatch\")\n",
        "            sys.exit(1)\n",
        "        if len(self.FINALS) != len(self.G_FINALS):\n",
        "            print(\"Error: character_FINALS and grapheme_FINALS length mismatch\")\n",
        "            sys.exit(1)\n",
        "\n",
        "    def loadTABLE(self, fn_grptable):\n",
        "        with open(fn_grptable, 'r', encoding='utf8') as fid:\n",
        "            grptable = fid.read().split('\\n')\n",
        "        if len(grptable) != 7:\n",
        "            print(\"Invalid table format\")\n",
        "            sys.exit(1)\n",
        "        return grptable\n",
        "\n",
        "    def check_syllable(self, x):\n",
        "        return 0xAC00 <= ord(x) <= 0xD7A3\n",
        "\n",
        "\n",
        "    def split_syllable_char(self, x):\n",
        "        if len(x) != 1:\n",
        "            raise ValueError(\"Input string must have exactly one character.\")\n",
        "\n",
        "        if not self.check_syllable(x):\n",
        "            raise ValueError(\n",
        "                \"Input string does not contain a valid Korean character.\")\n",
        "\n",
        "        diff = ord(x) - 0xAC00\n",
        "        _m = diff % 28\n",
        "        _d = (diff - _m) // 28\n",
        "\n",
        "        initial_index = _d // 21\n",
        "        medial_index = _d % 21\n",
        "        final_index = _m\n",
        "\n",
        "        if not final_index:\n",
        "            result_cha = (self.INITIALS[initial_index], self.MEDIALS[medial_index])\n",
        "            result_grp = (self.G_INITIALS[initial_index], self.G_MEDIALS[medial_index], 'X')\n",
        "        else:\n",
        "            result_cha = (\n",
        "                self.INITIALS[initial_index], self.MEDIALS[medial_index],\n",
        "                self.FINALS[final_index - 1])\n",
        "            result_grp = (\n",
        "                self.G_INITIALS[initial_index], self.G_MEDIALS[medial_index],\n",
        "                self.G_FINALS[final_index - 1])\n",
        "\n",
        "        return result_cha, result_grp\n",
        "\n",
        "    def split_syllables(self, string):\n",
        "\n",
        "        new_chracter = \"\"\n",
        "        new_grapheme = \"\"\n",
        "        for c in string:\n",
        "            if not self.check_syllable(c):\n",
        "                if (c in self.SPECIALS) or (c == ' '):\n",
        "                #if (c in self.SPECIALS):\n",
        "                    new_c = c\n",
        "                    new_g = c\n",
        "                else:\n",
        "                    new_c = ''\n",
        "                    new_g = ''\n",
        "                    self.flag_specials = True\n",
        "\n",
        "            else:\n",
        "                [c_sent, g_sent] = self.split_syllable_char(c)\n",
        "                new_c = \"\".join(c_sent)\n",
        "                new_g = \"\".join(g_sent)\n",
        "            new_chracter += new_c\n",
        "            new_grapheme += new_g\n",
        "\n",
        "        return new_chracter, new_grapheme\n",
        "\n",
        "def custom_english_cleaners(text):\n",
        "    _whitespace_re = re.compile(r'\\s+')\n",
        "    '''Custom pipeline for English text, including number and abbreviation expansion.'''\n",
        "    text = unidecode(text)\n",
        "    text = re.sub(_whitespace_re, ' ', text)\n",
        "    return text\n",
        "\n",
        "text2grp = Text2Grp(\"downloads/ptFastSpeech/data/grp/table.txt\")\n",
        "\n",
        "def frontend(text):\n",
        "    \"\"\"Clean text and then convert to id sequence.\"\"\"\n",
        "    _, grp = text2grp.split_syllables(text)\n",
        "    text = custom_english_cleaners(grp)\n",
        "    #print(f\"Cleaned text: {text}\")\n",
        "    charseq = list(text)\n",
        "    if not charseq[-1] in [',', '.',' !', '?']:\n",
        "        charseq += '.'\n",
        "    idseq = []\n",
        "    for c in charseq:\n",
        "        if c.isspace():\n",
        "            idseq += [char_to_id[\"<space>\"]]\n",
        "        elif c not in char_to_id.keys():\n",
        "            idseq += [char_to_id[\"<unk>\"]]\n",
        "        else:\n",
        "            idseq += [char_to_id[c]]\n",
        "    idseq += [idim - 1]  # <eos>\n",
        "    return torch.LongTensor(idseq).view(-1).to(device)\n",
        "\n",
        "from espnet.utils.cli_readers import file_reader_helper\n",
        "emotion_scp = 'scp:downloads/ptFastSpeech/exp/EMO_char_train_no_dev_pytorch_train_fastspeech.sgst2.spkid/outputs_model.loss.best_decode_stemb1.0/EMO_char_train_no_dev/emotion.scp'\n",
        "\n",
        "stemb_dict_f = {}\n",
        "stemb_dict_m = {}\n",
        "for idx, (utt_id, stemb1) in enumerate(file_reader_helper(emotion_scp, 'mat'), 1):\n",
        "    if utt_id[0] == 'f':\n",
        "        stemb_dict_f[utt_id[2:]] = stemb1\n",
        "    elif utt_id[0] == 'm':\n",
        "        stemb_dict_m[utt_id[2:]] = stemb1\n",
        "\n",
        "female_spk_list = ['0', '1', '2', '3', '4', '10' , '11', '12', '13', '14']\n",
        "male_spk_list = ['5', '6', '7', '8', '9', '15', '16', '17', '18', '19']\n",
        "spemb_list = female_spk_list + male_spk_list\n",
        "\n",
        "\n",
        "\n",
        "print(\"Now ready to synthesize!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now ready to synthesize!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AacD_RerASiO"
      },
      "source": [
        "## Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gGRzrjyudWF",
        "outputId": "b5ed796e-faf8-42a0-8b58-f638d459f44b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        "import time\n",
        "print(\"문장을 입력해주세요.\")\n",
        "input_text = input()\n",
        "print(\"화자 번호를 입력해주세요. *Option* 여자: 0~4, 10~14 / 남자: 5~9, 15~19\")\n",
        "input_speaker = input()\n",
        "print(\"감정을 입력해주세요. *Option* neutral, happy, sad, angry\")\n",
        "input_emotion = input()\n",
        "print(\"감정 세기를 입력해주세요. *Option* 0.5(약하게), 1.0(적당하게), 2.0(세게)\")\n",
        "input_weight = input()\n",
        "\n",
        "pad_fn = torch.nn.ReplicationPad1d(\n",
        "    config[\"generator_params\"].get(\"aux_context_window\", 0))\n",
        "use_noise_input = vocoder_class == \"ParallelWaveGANGenerator\"\n",
        "with torch.no_grad():\n",
        "    start = time.time()\n",
        "    if input_speaker in female_spk_list:\n",
        "      stemb_dict_in = stemb_dict_f\n",
        "    elif input_speaker in male_spk_list:\n",
        "      stemb_dict_in = stemb_dict_m\n",
        "    spemb = torch.LongTensor([int(input_speaker)]).view(-1).to(device)\n",
        "    stemb = torch.FloatTensor(stemb_dict_in[input_emotion]).view(-1).to(device) * float(input_weight)\n",
        "    x = frontend(input_text)\n",
        "    c, _, _ = model.inference(x, None, inference_args, spemb=spemb, stemb=stemb)\n",
        "    xx_denorm = cmvn(c.cpu().numpy())\n",
        "    c = torch.FloatTensor(scaler.transform(xx_denorm))\n",
        "    c = pad_fn(c.unsqueeze(0).transpose(2, 1)).to(device)\n",
        "    xx = (c,)\n",
        "    if use_noise_input:\n",
        "        z_size = (1, 1, (c.size(2) - sum(pad_fn.padding)) * config[\"hop_size\"])\n",
        "        z = torch.randn(z_size).to(device)\n",
        "        xx = (z,) + xx\n",
        "    if config[\"generator_params\"][\"out_channels\"] == 1:\n",
        "        y = vocoder(*xx).view(-1)\n",
        "    else:\n",
        "        y = pqmf.synthesis(vocoder(*xx)).view(-1)    \n",
        "rtf = (time.time() - start) / (len(y) / config[\"sampling_rate\"])\n",
        "print(f\"RTF = {rtf:5f}\")\n",
        "\n",
        "from IPython.display import display, Audio\n",
        "display(Audio(y.view(-1).cpu().numpy(), rate=config[\"sampling_rate\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문장을 입력해주세요.\n",
            "안녕하세요.\n",
            "화자 번호를 입력해주세요. *Option* 여자: 0~4, 10~14 / 남자: 5~9, 15~19\n",
            "0\n",
            "감정을 입력해주세요. *Option* neutral, happy, sad, angry\n",
            "neutral\n",
            "감정 세기를 입력해주세요. *Option* 0.5(약하게), 1.0(적당하게), 2.0(세게)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}